<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>SafePicking: Learning Safe Object Extraction via Object-Level Mapping</title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <link rel="stylesheet" href="assets/css/main.css">

    <script async src="https://www.googletagmanager.com/gtag/js?id=G-J30ZNP92Q8"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-J30ZNP92Q8');
    </script>

    <meta property="og:url"           content="https://safepicking.wkentaro.com/" />
    <meta property="og:type"          content="website" />
    <meta property="og:title"         content="SafePicking: Learning Safe Object Extraction via Object-Level Mapping" />
    <meta property="og:description"   content="Robots need object-level scene understanding to manipulate objects while reasoning about contact, support, and occlusion among objects. Given a pile of objects, object recognition and reconstruction can identify the boundary of object instances, giving important cues as to how the objects form and support the pile. In this work, we present a system, \textbf{\textit{SafePicking}}, that integrates object-level mapping and learning-based motion planning to generate a motion that safely extracts occluded target objects from a pile. Planning is done by learning a deep Q-network that receives observations of predicted poses and a depth-based heightmap to output a motion trajectory, trained to maximize a safety metric reward. Our results show that the observation fusion of poses and depth-sensing gives both better performance and robustness to the model. We evaluate our methods using the YCB objects in both simulation and the real world, achieving safe object extraction from piles." />
    <meta property="og:image" content="https://safepicking.wkentaro.com/assets/img/teaser_vertical.png" />
  </head>
  <body>
    <div class="container-fluid">
      <div class="row">
        <div class="col-lg-8 offset-lg-2 col-md-12">

          <div class="text-center">
            <h1 class="mt-5"><b>SafePicking</b></h1>
            <h4 class="mt-4">Learning Safe Object Extraction via Object-Level Mapping</h4>
            <ul class="list-inline mt-4">
              <li class="list-inline-item"><a href="https://wkentaro.com" target="_blank">Kentaro Wada</a></li>
              <li class="list-inline-item ml-4"><a href="https://stepjam.github.io" target="_blank">Stephen James</a></li>
              <li class="list-inline-item ml-4"><a href="https://www.doc.ic.ac.uk/~ajd/" target="_blank">Andrew J. Davison</a></li>
              <li class="mt-2">
                <a href="https://www.imperial.ac.uk/dyson-robotics-lab/" target="_blank">Dyson Robotic Laboratory</a>,
                <a href="https://www.imperial.ac.uk/" target="_blank" class="ml-2">Imperial College London</a>
              </li>
            </ul>
            <ul class="list-inline mt-4">
              <li class="list-inline-item">
                <a href="https://arxiv.org/abs/2202.05832" target="_blank">Paper</a>
              </li>
              <li class="list-inline-item ml-4">
                <a href="https://youtu.be/ejjqiBqRRKo" target="_blank">Video</a>
              </li>
              <li class="list-inline-item ml-4">
                <a href="https://github.com/wkentaro/safepicking" target="_blank">Code</a>
              </li>
            </ul>
          </div>


          <div class="row mt-4">
            <div class="col-md-6 offset-md-1">
              <p>
                Robots need object-level scene understanding to manipulate objects while reasoning about contact, support, and occlusion among objects. Given a pile of objects, object recognition and reconstruction can identify the boundary of object instances, giving important cues as to how the objects form and support the pile.
              </p>
              <p>
              In this paper, we present a system, <u>SafePicking</u>, that integrates object-level mapping and learning-based motion planning to generate a motion that safely extracts occluded target objects from a pile. We train a deep Q-network to maximize a safety metric reward for motion generation with feeding as input estimated object poses and a depth-based heightmap.
              </p>
            </div>
            <div class="col-md-4 text-center">
              <img src="assets/img/teaser_vertical.png" class="img-fluid" width="90%">
            </div>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4 id="video">Overview Video (with audio)</h4>
            <div align="center" class="row mt-4">
              <div class="col-lg-8 offset-lg-2">
                <div class="video-container">
                  <iframe width="560" height="315" src="https://www.youtube.com/embed/ejjqiBqRRKo" frameborder="0" allowfullscreen></iframe>
                </div>

              </div>
            </div>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4 id="pipeline">Pipeline for object mapping and extraction</h4>
            <p>
            We present a pipeline that finds target objects and extract them from a cluttered environment such as an object pile. This pipeline consists of <u>1) object-level mapping with volumetric reconstruction and pose estimation of detected objects</u> with an on-board RGB-D camera, and <u>2) learning-based motion planning with a model that recursively generates end-effector relative motions</u> using estimated object poses and a raw depth observation in the form of a heightmap.
            </p>
            <img src="assets/img/pipeline.png" class="img-fluid">
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4 id="network">Network model for motion generation</h4>
            <p>
              We train a deep Q-network using a "safety metric reward" during object extraction. The network model receives fused observations of a depth-based heightmap and estimated object poses <u>to achieve high levels of success with object pose and robustness to estimation errors with the raw depth observation</u>. We feed end-effector relative transformations as evaluation actions, from which the best-scored action is selected as the next action.
            </p>
            <div class="col-lg-8 offset-lg-2">
              <img src="assets/img/network.png" class="img-fluid">
            </div>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4 id="network">Training in simulation</h4>
            <p>
              We train the model in physics simulation using the CAD models of known objects. <u>The safety metric reward is computed as the sum of the translations of objects</u> in a pile, which incorporates undesirable consequences such as dropping and sliding.
            </p>
            <img src="assets/img/learned_in_simulation.gif" class="img-fluid">
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4>Baseline comparison in safe object extraction</h4>
            <p>
              Our proposed method, SafePicking, <u>surpasses all of the baseline methods</u>: Naive, joint interpolation to a reset pose; Heuristic, a straight motion to lift a grasped object up; RRT-Connect, motion planning using ground-truth object poses.
            </p>
            <div class="table-responsive">
              <table class="table table-sm table-bordered text-center mb-0" style="font-size: 13px;">
                <caption class="pb-0">We test the methods in 600 unseen environments in simulation.</caption>
                <tr>
                  <th rowspan="2" class="align-middle">Method</th>
                  <th rowspan="2" class="align-middle">Observation</th>
                  <th colspan="2">Safety metric</th>
                </tr>
                <tr style="border-bottom: solid 3px;">
                  <th>translation↓</th>
                  <th>velocity↓</th>
                </tr>
                <tr>
                  <td>Naive</td>
                  <td>-</td>
                  <td>0.701</td>
                  <td>1.919</td>
                </tr>
                <tr>
                  <td>Heuristic</td>
                  <td>-</td>
                  <td>0.578</td>
                  <td>1.624</td>
                </tr>
                <tr>
                  <td>RRT-Connect</td>
                  <td>pose</td>
                  <td>0.520</td>
                  <td>1.643</td>
                </tr>
                <tr>
                  <td>SafePicking</td>
                  <td>pose, heightmap</td>
                  <td class="table-active">0.465</td>
                  <td class="table-active">1.419</td>
                </tr>
              </table>
            </div>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4>Model comparison using different observations as input</h4>
            <p>
              Our proposed model shows <u>the best result when given both object poses and a raw depth observation as input</u>. The pose information enables the model to generate better motions using more complete semantic information, and the raw depth information enables it to be robust to estimation errors (pose errors; mis-detection) as a less processed scene observation.
            </p>
            <div class="table-responsive">
              <table class="table table-sm table-bordered text-center" style="font-size: 13px;">
                <caption>We test the models in 600 unseen environments in simulation.</caption>
                <tr>
                  <th rowspan="2" class="align-middle">Model variant</th>
                  <th rowspan="2" class="align-middle">Observation</th>
                  <th rowspan="2" class="align-middle">Errors in<br/>object poses</th>
                  <th colspan="2">Safety metric</th>
                </tr>
                <tr style="border-bottom: solid 3px;">
                  <th>translation↓</th>
                  <th>velocity↓</th>
                </tr>
                <tr>
                  <td>Raw-only</td>
                  <td>heightmap</td>
                  <td rowspan="3" class="align-middle">no</td>
                  <td>0.507</td>
                  <td>1.491</td>
                </tr>
                <tr>
                  <td>Pose-only</td>
                  <td>pose</td>
                  <td>0.477</td>
                  <td>1.430</td>
                </tr>
                <tr>
                  <td>Pose+Raw</td>
                  <td>pose, heightmap</td>
                  <td class="table-active">0.465</td>
                  <td class="table-active">1.419</td>
                </tr>
                <tr style="border-top: solid 1.5px;">
                  <td>Pose-only</td>
                  <td>pose</td>
                  <td rowspan="2" class="align-middle">yes</td>
                  <td>0.487</td>
                  <td>1.449</td>
                </tr>
                <tr>
                  <td>Pose+Raw</td>
                  <td>pose, heightmap</td>
                  <td class="table-active">0.465</td>
                  <td class="table-active">1.433</td>
                </tr>
              </table>
            </div>
            <img src="assets/img/evaluation_in_sim.png" class="img-fluid">
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4>Real-world experiments</h4>
            <div class="row mt-4">
              <div class="col-lg-6">
                <h5>Full pipeline demonstration</h5>
                <a href="https://youtu.be/ejjqiBqRRKo?t=29">
                  <img src="assets/img/whole_pipeline.gif" class="img-fluid">
                </a>
              </div>
              <div class="col-lg-6">
                <h5>Heuristic vs. Learned</h5>
                <a href="https://youtu.be/ejjqiBqRRKo?t=51">
                  <img src="assets/img/vs_heuristic.gif" class="img-fluid">
                </a>
              </div>
            </div>
            <div class="row mt-3">
              <div class="col-lg-6">
                <h5>Learned model ablation</h5>
                <a href="https://youtu.be/ejjqiBqRRKo?t=114">
                  <img src="assets/img/real_ablation.gif" class="img-fluid">
                </a>
              </div>
              <div class="col-lg-6">
                <h5>Adaptation to pile changes</h5>
                <a href="https://youtu.be/ejjqiBqRRKo?t=145">
                  <img src="assets/img/adjustment.gif" class="img-fluid">
                </a>
              </div>
            </div>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4 id="video">Paper</h4>
            <a href="https://arxiv.org/abs/2202.05832" target="_blank">
              <img src="assets/img/paper.jpg" class="img-fluid">
            </a>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4 id="bibtex">Bibtex</h4>
            <pre>
  @inproceedings{Wada:etal:ICRA2022a,
    title={{SafePicking}: Learning Safe Object Extraction via Object-Level Mapping},
    author={Kentaro Wada and Stephen James and Andrew J. Davison},
    booktitle={IEEE International Conference on Robotics and Automation (ICRA)},
    year={2022},
  }
            </pre>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1 mb-5">
            <h4 id="contact">Contact</h4>
            <p>
              If you have any questions, please feel free to contact
              <a href="https://wkentaro.com" target="_blank">Kentaro Wada</a>.
            </p>
          </div>

        </div>
      </div>
    </div>
  </body>
</html>
